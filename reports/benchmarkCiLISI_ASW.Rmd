---
title: "Benchmark of all integration tools with original updated scib pipeline"
author: "Leonard Herault"
date: '2022-12-09'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Major pipeline updates :

-   Adding STACAS and semi supervised STACAS
-   Using embedding output (PCA computed on scaled integrated data with Seurat) for R based methods
-   Integration tools packages updated
-   Embedding space for integration with a size fixed to 30 for all tools (Reduced space or bottleneck layer of autoencoders)

## libraries

```{r cars}
library(ggplot2)
library(Seurat)
library(ggrepel)
library(tibble)
library(RColorBrewer)
library(dynutils)
library(stringr)
library(Hmisc)
library(plyr)
library(ggpubr)
source("knit_table.R") 
source("R_fun_report.R")
```

```{r}
ids_RNA <- c("human_pancreas", "lung_atlas", "immune_cell_hum","tcells_atlas")
labels_RNA <- c("Pancreas", "Lung", "Immune", 'T cells')
```

```{r}
# library(STACAS)
# library(Seurat)
# library(scIntegrationMetrics)
```

This report requires the files :

-   `../data/metrics_R.csv`
-   `../data/metrics.csv`

generated with the snakemake pipeline that are available in the `results` folder of the repository (you can simply copy paste them in a folder data at the root of the repository).

It also required the files (already available here in the repository) :

-   `./scalability_score_memory_revision.csv`
-   `./scalability_score_memory_revision.csv`

generated by the jupyter notebook `scalability_assessment.ipynb` from the file `../data/benchmarks.csv` (we also provide it in the results folder of this repository)

## Metrics loading

```{r}
metricsR <- read.csv("../data/metrics_R.csv",row.names = 1)

scenarios <- str_split_fixed(rownames(metricsR),pattern = "/",n=6)
head(scenarios)

scenarios <- scenarios[,-1]
colnames(scenarios) <- c("task","folder","scaling","features","method")
metricsR <- cbind(scenarios,metricsR)

metricsR$method <- str_split_fixed(metricsR$method,pattern = "/",n=2)[,2]
metricsR$output <- str_split_fixed(metricsR$method,pattern = "_",n=2)[,2]
metricsR$tool <- str_split_fixed(metricsR$method,pattern = "_",n=2)[,1]

head(metricsR)
```

```{r}
metrics <- read.csv("../data/metrics.csv",row.names = 1)
metricsShort <- metrics[,!colnames(metrics) %in% c("hvg_overlap","cell_cycle_conservation")]
write.csv(metricsShort,"../data/metricsShort.csv")

scenarios <- str_split_fixed(rownames(metrics),pattern = "/",n=6)
head(scenarios)

scenarios <- scenarios[,-1]
colnames(scenarios) <- c("task","folder","scaling","features","method")
metrics <- cbind(scenarios,metrics)

metrics$output <- str_split_fixed(metrics$method,pattern = "_",n=2)[,2]
metrics$tool <- str_split_fixed(metrics$method,pattern = "_",n=2)[,1]

head(metrics)

```

Combine metrics computed in R and python

```{r}
rownames(metricsR) <- gsub(rownames(metricsR),pattern = "/R/",replacement = "/")
metrics <- cbind(metrics, metricsR[rownames(metrics),c("CiLISI","CiLISI_means")])
metrics$ASW_label_raw <- 2*metrics$ASW_label - 1

metrics$methodLong <- paste(metrics$method,metrics$scaling,sep = "_")

```

# CiLISI vs ASW_label

```{r fig.width=3.5,fig.width=3.5}
methodToCompare <- c("STACAS_embed_unscaled","STACAS_embed_scaled",
                     "semiSupSTACAS_embed_unscaled","semiSupSTACAS_embed_scaled",
                     "seuratrpca_embed_unscaled","seuratrpca_embed_scaled",
                     "seurat_embed_unscaled","seurat_embed_scaled",
                     "fastmnn_embed_scaled","fastmnn_embed_unscaled",
                     "harmony_embed_unscaled","harmony_embed_scaled",
                      "scvi_embed_unscaled",
                     "scanorama_embed_scaled","scanorama_embed_unscaled",
                     "scanvi_embed_unscaled",
                     "combat_full_scaled","combat_full_unscaled",
                     "scgen_full_unscaled","unintegrated_full_unscaled"
                     )
for (t in unique(metrics$task)) {
  plot(ggplot(metrics[metrics$task == t & metrics$methodLong %in% methodToCompare,],
         aes(x = CiLISI,color=tool,y=ASW_label_raw,shape= scaling)) +geom_point() + facet_wrap("~task",scales = "free",ncol = 2) +
  geom_label_repel(aes(label = paste0(tool,"_",scaling)),
                  box.padding   = 0.35, 
                  point.padding = 0.5,
                  segment.color = 'grey50') + NoLegend())
}
```

## Saving PDF

```{r}
dir.create("pdf/CiLISI_vs_ASW_label",recursive = T)
for (t in unique(metrics$task)) {
  pdf(file = paste0("pdf/CiLISI_vs_ASW_label/",t,"_CiLISI_vs_ASW_label.pdf"),width = 8, height = 8)
  plot(ggplot(metrics[metrics$task == t & metrics$methodLong %in% methodToCompare,],
         aes(x = CiLISI,color=tool,y=ASW_label_raw,shape= scaling)) +geom_point() + facet_wrap("~task",scales = "free",ncol = 2) +
  geom_label_repel(aes(label = paste0(tool,"_",scaling)),
                  box.padding   = 0.35, 
                  point.padding = 0.5,
                  segment.color = 'grey50') + NoLegend())
  dev.off()
}
```

## Correlations between batch mixing metrics.

Pearson correlations :

```{r}
ggplot(metrics[metrics$methodLong %in% methodToCompare,],aes(x=kBET,y=CiLISI)) + geom_point() + facet_wrap("~task") + stat_cor()

ggplot(metrics[metrics$methodLong %in% methodToCompare,],aes(x=iLISI,y=CiLISI)) + geom_point() + facet_wrap("~task") + stat_cor()

```

Spearman correlations:

```{r}
ggplot(metrics[metrics$methodLong %in% methodToCompare,],aes(x=kBET,y=CiLISI)) + geom_point() + facet_wrap("~task") + stat_cor(method = "spearman")

ggplot(metrics[metrics$methodLong %in% methodToCompare,],aes(x=iLISI,y=CiLISI)) + geom_point() + facet_wrap("~task") + stat_cor(method = "spearman")
```

Check if silhouette label computed in R (raw) and python (scaled between 0 and 1) are consistent.

```{r}
ggplot(data = metrics[metrics$methodLong %in% methodToCompare,],aes(x= ASW_label_raw,y=ASW_label)) + geom_point() + xlim(c(-1,1)) + ylim(c(0,1))
```

# Ranking of integration methods

## Only CiLISI and ASW_label

```{r}

metricsCiLISIlabelASW_all <- metrics[,c("CiLISI","ASW_label")]

colnames(metricsCiLISIlabelASW_all) <- c("iLISI","ASW_label")
dir.create("CiLISIlabelASW_all/singleTask",recursive = T)
dir.create("CiLISIlabelASW_all/best")

write.csv(metricsCiLISIlabelASW_all,"CiLISIlabelASW_all/metrics.csv",col.names = T)


plotBestMethodsRNA(
  csv_metrics_path = "CiLISIlabelASW_all/metrics.csv",
  outdir = "CiLISIlabelASW_all",
  # csv_usability_path = "./data/usability4bestMethods.csv",
  csv_scalability_time_path = "./scalability_score_time_revision.csv",
  csv_scalability_memory_path = "./scalability_score_memory_revision.csv",
  ids_RNA = ids_RNA,
  #ids_simulation = c("simulations_1_1", "simulations_2"),  
  labels_RNA = labels_RNA,
  #labels_simulation = c("Sim 1", "Sim 2"), 
  weight_batch = 0.4,
  keepBest = F, keepBestOutput = F
)


plotBestMethodsRNA(
  csv_metrics_path = "CiLISIlabelASW_all/metrics.csv",
  outdir = "CiLISIlabelASW_all/best",
  # csv_usability_path = "./data/usability4bestMethods.csv",
  csv_scalability_time_path = "./scalability_score_time_revision.csv",
  csv_scalability_memory_path = "./scalability_score_memory_revision.csv",
  ids_RNA = ids_RNA,
  #ids_simulation = c("simulations_1_1", "simulations_2"),  
  labels_RNA = labels_RNA,
  #labels_simulation = c("Sim 1", "Sim 2"), 
  weight_batch = 0.4,
  keepBest = T, keepBestOutput = T
)



plotSingleTaskRNA("CiLISIlabelASW_all/metrics.csv",outdir = "CiLISIlabelASW_all/singleTask")

```

As you can see in the plot generated in `CiLISIlabelASW_all/singleTask` gene matrix output (`"*_full"`) for R based methods score badly, especially fastmnn (maybe because of the use of 30 PCs as input). This skew the results by narrowing the gap between the other methods after the scaling.

Thus, we discard gene matrix output (`"*_full"`) for R based methods.

```{r}
metricsCiLISIlabelASW <- metrics[!metrics$method %in% c("seuratrpca_full","semiSupSTACAS_full","STACAS_full","seurat_full","fastmnn_full"),]
metricsCiLISIlabelASW <- metricsCiLISIlabelASW[,c("CiLISI","ASW_label")]

#metricsCiLISIlabelASW <- metricsCiLISIlabelASW[,c("CiLISI","ASW_label_raw")] 

#metricsCiLISIlabelASW$ASW_label_raw[metricsCiLISIlabelASW$ASW_label_raw < 0] <- 0

colnames(metricsCiLISIlabelASW) <- c("iLISI","ASW_label")
dir.create("CiLISIlabelASW/singleTask",recursive = T)
dir.create("CiLISIlabelASW/best")

write.csv(metricsCiLISIlabelASW,"CiLISIlabelASW/metrics.csv",col.names = T)


plotBestMethodsRNA(
  csv_metrics_path = "CiLISIlabelASW/metrics.csv",
  outdir = "CiLISIlabelASW",
  # csv_usability_path = "./data/usability4bestMethods.csv",
  csv_scalability_time_path = "./scalability_score_time_revision.csv",
  csv_scalability_memory_path = "./scalability_score_memory_revision.csv",
  ids_RNA = ids_RNA,
  #ids_simulation = c("simulations_1_1", "simulations_2"),  
  labels_RNA = labels_RNA,
  #labels_simulation = c("Sim 1", "Sim 2"), 
  weight_batch = 0.4,
  keepBest = F, keepBestOutput = F
)


BestMethodsCiLISIASW0 <- plotBestMethodsRNA(
  csv_metrics_path = "CiLISIlabelASW/metrics.csv",
  outdir = "CiLISIlabelASW/best",
  # csv_usability_path = "./data/usability4bestMethods.csv",
  csv_scalability_time_path = "./scalability_score_time_revision.csv",
  csv_scalability_memory_path = "./scalability_score_memory_revision.csv",
  ids_RNA = ids_RNA,
  #ids_simulation = c("simulations_1_1", "simulations_2"),  
  labels_RNA = labels_RNA,
  #labels_simulation = c("Sim 1", "Sim 2"), 
  weight_batch = 0.4,
  keepBest = T, keepBestOutput = T
)



plotSingleTaskRNA("CiLISIlabelASW/metrics.csv",outdir = "CiLISIlabelASW/singleTask")

```

## All original metrics except HVG overlap

We chose not to consider HVG overlap as it can be computed only for a minority of methods outputting a corrected matrix.

```{r}
oriMetrics <- colnames(read.csv("../data/metrics.csv",row.names = 1))
oriMetrics <- oriMetrics[oriMetrics != "hvg_overlap"]
 
metricsMethodOK <- metrics[!metrics$method %in% c("seuratrpca_full","semiSupSTACAS_full","STACAS_full","seurat_full","fastmnn_full"),
                           oriMetrics]

dir.create("oriMetrics/singleTask",recursive = T)
dir.create("oriMetrics/best")

write.csv(metricsMethodOK,"oriMetrics/metrics.csv",col.names = T)


plotBestMethodsRNA(
  csv_metrics_path = "oriMetrics/metrics.csv",
  outdir = "oriMetrics",
  # csv_usability_path = "./data/usability4bestMethods.csv",
  csv_scalability_time_path = "./scalability_score_time_revision.csv",
  csv_scalability_memory_path = "./scalability_score_memory_revision.csv",
  ids_RNA = ids_RNA,
  #ids_simulation = c("simulations_1_1", "simulations_2"),  
  labels_RNA = labels_RNA,
  #labels_simulation = c("Sim 1", "Sim 2"), 
  weight_batch = 0.4,
  keepBest = F, keepBestOutput = F
)


plotBestMethodsRNA(
  csv_metrics_path = "oriMetrics/metrics.csv",
  outdir = "oriMetrics/best",
  # csv_usability_path = "./data/usability4bestMethods.csv",
  csv_scalability_time_path = "./scalability_score_time_revision.csv",
  csv_scalability_memory_path = "./scalability_score_memory_revision.csv",
  ids_RNA = ids_RNA,
  #ids_simulation = c("simulations_1_1", "simulations_2"),  
  labels_RNA = labels_RNA,
  #labels_simulation = c("Sim 1", "Sim 2"), 
  weight_batch = 0.4,
  keepBest = T, keepBestOutput = T
)



plotSingleTaskRNA("oriMetrics/metrics.csv",outdir = "oriMetrics/singleTask")

```

# Scalability results

Please not that scvi, scanvi, scgen have been run using only cpu although these methods have a gpu support.

```{r}
BestMethodsCiLISIASW <- BestMethodsCiLISIASW0

BestMethodsCiLISIASW$Method <- plyr::mapvalues(BestMethodsCiLISIASW$Method,
                               to = c("seurat", "seuratrpca", "bbknn", "scvi", "liger", "combat", "fastmnn", "scanvi", "scgen","STACAS","semiSupSTACAS","scanorama","unintegrated","harmony","combat"), 
                               from = c("Seurat v4 CCA", "Seurat v4 RPCA", "BBKNN", "scVI", "LIGER", "ComBat", "fastMNN", "scANVI*", "scGen*","STACAS","ssSTACAS*","Scanorama","Unintegrated","Harmony","ComBat"))

BestMethodsCiLISIASW$methodLong <- paste0(BestMethodsCiLISIASW$Method,"_",BestMethodsCiLISIASW$Output,"_",BestMethodsCiLISIASW$Scaling)

# gene matrix output is abbreviated full in metrics data frame
BestMethodsCiLISIASW$methodLong <- gsub(BestMethodsCiLISIASW$methodLong,pattern = "gene",replacement = "full")  

BestMethodsCiLISIASW <- metrics[metrics$methodLong %in% BestMethodsCiLISIASW$methodLong,]

BestMethodsCiLISIASW$matchScalability <-  paste0(BestMethodsCiLISIASW$task,"_",
                                                 BestMethodsCiLISIASW$tool,"_",
                                                 BestMethodsCiLISIASW$features,"_",
                                                 BestMethodsCiLISIASW$scaling)


rownames(BestMethodsCiLISIASW) <- BestMethodsCiLISIASW$matchScalability
scalability <- read.csv("./benchmark_meta.csv")
rownames(scalability) <- paste(scalability$scenario1,
                               scalability$method,
                                 scalability$features,
                                 scalability$scale,sep = "_")

bestMethodScalability <- BestMethodsCiLISIASW[BestMethodsCiLISIASW$tool != "Unintegrated",c("tool","methodLong")]
bestMethodScalability <- cbind(bestMethodScalability,scalability[rownames(bestMethodScalability),])



bestMethodScalability <- bestMethodScalability[order(bestMethodScalability$methodLong),]
bestMethodScalability$tool <- factor(bestMethodScalability$tool,levels = unique(bestMethodScalability$tool))

matchColor <- scales::hue_pal()(12)
names(matchColor) <- levels(bestMethodScalability$tool)

## order
bestMethodScalability$methodLong <- factor(bestMethodScalability$methodLong,levels = rev(BestMethodsCiLISIASW0$methodLong))

bestMethodScalability <- bestMethodScalability[order(bestMethodScalability$methodLong),]
bestMethodScalability$tool <- factor(bestMethodScalability$tool,levels = unique(bestMethodScalability$tool))



p1 <- ggplot(data = bestMethodScalability,aes(x=tool,y = cpu_time/60, fill = tool)) +geom_boxplot() + coord_flip() + NoLegend() + scale_fill_manual(values = matchColor) + ylab('cpu time (min)')

p2 <- ggplot(data = bestMethodScalability,aes(x=tool,y = max_pss/10^3, fill = tool)) +geom_boxplot() + coord_flip() + NoLegend() + scale_fill_manual(values = matchColor) + ylab('memory (GB)')

p1+p2
```
