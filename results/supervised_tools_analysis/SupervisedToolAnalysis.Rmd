---
title: "Benchmark of supervised integration tools"
author: "Leonard Herault"
date: '2022-12-09'
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,comment = FALSE,warning = FALSE)
```

## libraries

```{r cars}
library(ggplot2)
library(Seurat)
library(ggrepel)
library(tibble)
library(RColorBrewer)
library(dynutils)
library(stringr)
library(Hmisc)
library(plyr)
# library(STACAS)
# library(Seurat)
# library(scIntegrationMetrics)
library(anndata)
```


This report requires the files :

-   `./metrics_R.csv`
-   `./metrics.csv`

generated with the snakemake pipeline using the config file `configs/test_supervised_methods-R4.1.yaml`.
First we copy them in the current directory (already done on the repository)

```{r}
#Move to directory of this script
#setwd("./results/supervised_tools_analysis")


if (!file.exists("./metrics_R.csv")) {
  file.copy("../../supervised_tool_analysis/metrics_R.csv","./")
}

if (!file.exists("./metrics.csv")) {
  file.copy("../../supervised_tool_analysis/metrics.csv","./")
}
```

It also requires the files
-   `../original_annotations/metrics_R.csv`
-   `../original_annotations/metrics.csv`

generated with the snakemake pipeline using the config file `configs/test_original_annotations-R4.1.yaml`.
First we copy them in the `results/original_annotations` directory (already done in the repository)

```{r}
if (!file.exists("../original_annotations/metrics_R.csv")) {
  file.copy("../../original_annotations/metrics_R.csv","../original_annotations/metrics_R.csv")
}

if (!file.exists("../original_annotations/metrics.csv")) {
  file.copy("../../original_annotations/metrics.csv","../original_annotations/metrics.csv")
}
```





## Integration metric loading

```{r}
metricsR <- read.csv("metrics_R.csv",row.names = 1)

scenarios <- str_split_fixed(rownames(metricsR),pattern = "/",n=7)
#head(scenarios)

scenarios <- scenarios[,-1]
colnames(scenarios) <- c("task","annotations","folder","scaling","features","method")
metricsR <- cbind(scenarios,metricsR)

metricsR$method <- str_split_fixed(metricsR$method,pattern = "/",n=2)[,2]
metricsR$output <- str_split_fixed(metricsR$method,pattern = "_",n=2)[,2]
metricsR$tool <- str_split_fixed(metricsR$method,pattern = "_",n=2)[,1]

metricsR_unaltered <- read.csv("../original_annotations//metrics_R.csv",row.names = 1)

scenarios_unaltered <- str_split_fixed(rownames(metricsR_unaltered),pattern = "/",n=7)
#head(scenarios_unaltered)

scenarios_unaltered <- data.frame(scenarios_unaltered[,-1])

colnames(scenarios_unaltered) <- c("task","annotations","folder","scaling","features","method")
scenarios_unaltered$method <- str_split_fixed(scenarios_unaltered$method,pattern = "/",n=2)[,2]



keptIndex <- which(scenarios_unaltered$scaling %in% c("unscaled") &
                     scenarios_unaltered$method %in% c("scanvi_embed","scvi_embed","semiSupSTACAS_embed","STACAS_embed","scgen_full","unintegrated_full"))

rn <- rownames(metricsR_unaltered)[keptIndex]

metricsR_unaltered <- cbind(scenarios_unaltered[keptIndex,],metricsR_unaltered[keptIndex,])
rownames(metricsR_unaltered) <- rn

metricsR_unaltered$output <- str_split_fixed(metricsR_unaltered$method,pattern = "_",n=2)[,2]
metricsR_unaltered$tool <- str_split_fixed(metricsR_unaltered$method,pattern = "_",n=2)[,1]


metricsR <- rbind(metricsR,metricsR_unaltered[,colnames(metricsR)])
```




```{r}
metrics <- read.csv("metrics.csv",row.names = 1)

scenarios <- str_split_fixed(rownames(metrics),pattern = "/",n=7)
#head(scenarios)

scenarios <- scenarios[,-1]
colnames(scenarios) <- c("task","annotations","folder","scaling","features","method")
metrics <- cbind(scenarios,metrics)

#metrics$method <- str_split_fixed(metrics$method,pattern = "/",n=2)[,2]
metrics$output <- str_split_fixed(metrics$method,pattern = "_",n=2)[,2]
metrics$tool <- str_split_fixed(metrics$method,pattern = "_",n=2)[,1]

metrics_unaltered <- read.csv("../original_annotations/metrics.csv",row.names = 1)

scenarios_unaltered <- str_split_fixed(rownames(metrics_unaltered),pattern = "/",n=7)
#head(scenarios_unaltered)

scenarios_unaltered <- data.frame(scenarios_unaltered[,-1])

colnames(scenarios_unaltered) <- c("task","annotations","folder","scaling","features","method")
#scenarios_unaltered$method <- str_split_fixed(scenarios_unaltered$method,pattern = "/",n=2)[,2]



keptIndex <- which(scenarios_unaltered$scaling %in% c("unscaled") &
                     scenarios_unaltered$method %in% c("scanvi_embed","scvi_embed","semiSupSTACAS_embed","STACAS_embed","scgen_full","unintegrated_full"))

rn <- rownames(metrics_unaltered)[keptIndex]


metrics_unaltered <- cbind(scenarios_unaltered[keptIndex,],metrics_unaltered[keptIndex,])

rownames(metrics_unaltered) <- rn

metrics_unaltered$output <- str_split_fixed(metrics_unaltered$method,pattern = "_",n=2)[,2]
metrics_unaltered$tool <- str_split_fixed(metrics_unaltered$method,pattern = "_",n=2)[,1]


metrics <- rbind(metrics,metrics_unaltered[,colnames(metrics)])
```


Compared to first analysis results with original annotations, we only analysed unscaled scenarios
```{r}
#TO DO will be removed from results and config file as it is now in the final_benchmark config
metrics <- metrics[metrics$scaling == "unscaled",]
metricsR <- metricsR[metricsR$scaling == "unscaled",]
```

We don't analyze the mixing between partial and shuffled annotations in this analysis

```{r}
metrics <- metrics[metrics$annotations != "unknown_15_shuffled_20",]
metrics <- metrics[metrics$annotations != "unknown_15_shuffled_20",]

```


# Label shuffling analysis

## ASW label score

```{r fig.width=10, fig.height=2.5}
pdir <- "pdf/overfitting"
dir.create(pdir,recursive = T)

palette <- c("#FF3333","#FF9999","#FFFF00","#996633","#FFCC66","#99CCCC")
names(palette) <- c("semiSupSTACAS_embed","scanvi_embed","scgen_full","STACAS_embed","scvi_embed","unintegrated_full")
shapes <- c(19,19,19,9,9,9)
names(shapes) <- names(palette)

metrics$annotations <- factor(metrics$annotations,levels = c("original", "broad",
                                                             "shuffled_10","shuffled_20","shuffled_50","shuffled_100",
                                                             "unknown_10", "unknown_20","unknown_50","unknown_100"))
metrics$ASW_label_raw <- 2*metrics$ASW_label-1

data <- metrics[grepl(metrics$annotations,pattern = "shuffled|original"),]
data$method <- factor(data$method, levels=names(palette))

ggplot(data, aes(x = annotations,color=method,y=ASW_label_raw,group = method)) +
  geom_point(aes(shape=method)) + geom_line() +  facet_wrap('~task', scales="free", ncol=4) + theme_bw() +
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  scale_color_manual(values=palette) +
  scale_shape_manual(values=shapes)

ggsave(sprintf("%s/effect_of_shuffling_ss.pdf", pdir), height=2.5, width=10)
```

## CiLISI score
```{r fig.width=10, fig.height=2.5}
metricsR$annotations <- factor(metricsR$annotations,levels = c("original", "broad",
                                                               "shuffled_10","shuffled_20","shuffled_50","shuffled_100",
                                                               "unknown_10", "unknown_20","unknown_50","unknown_100"))

data <- metricsR[grepl(metricsR$annotations,pattern = "shuffled|original"),]

ggplot(data, aes(x = annotations,color=method,y=CiLISI,group = method)) +
  geom_point(aes(shape=method)) + geom_line() +  facet_wrap('~task', scales="free", ncol=4) + theme_bw() +
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  scale_color_manual(values=palette) +
  scale_shape_manual(values=shapes)

ggsave(sprintf("%s/effect_of_shuffling_CiLISI.pdf", pdir), height=2.5, width=10)
```



# Unknown labels analysis

## ASW label score

```{r fig.width=10, fig.height=2.5}
data <- metrics[grepl(metrics$annotations,pattern = "unknown|original"),]
data$method <- factor(data$method, levels=names(palette))

ggplot(data, aes(x = annotations,color=method,y=ASW_label_raw,group = method)) +
  geom_point(aes(shape=method)) + geom_line() +  facet_wrap('~task', scales="free", ncol=4) + theme_bw() +
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  scale_color_manual(values=palette) +
  scale_shape_manual(values=shapes)

ggsave(sprintf("%s/effect_of_unknown_ss.pdf", pdir), height=2.5, width=10)
```

## CiLISI score
```{r fig.width=10, fig.height=2.5}
metricsR$annotations <- factor(metricsR$annotations,levels = c("original",
                                                               "unknown_10", "unknown_20","unknown_50","unknown_100"))

data <- metricsR[grepl(metricsR$annotations,pattern = "unknown|original"),]

ggplot(data, aes(x = annotations,color=method,y=CiLISI,group = method)) +
  geom_point(aes(shape=method)) + geom_line() +  facet_wrap('~task', scales="free", ncol=4) + theme_bw() +
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  scale_color_manual(values=palette) +
  scale_shape_manual(values=shapes)

ggsave(sprintf("%s/effect_of_unknown_CiLISI.pdf", pdir), height=2.5, width=10)
```


# Broad annotation analysis

None (no altered annotations) refers to the original fine annotations.
Please note that for lung and pancreas tasks broad and fine annotations are the same.

## ASW label score

```{r}
data <- metrics[grepl(metrics$annotations,pattern = "broad|original")&
                 metrics$task %in% c('immune_cell_hum','tcells_atlas'),]

ggplot(data,
       aes(x = annotations,color=method,y=ASW_label_raw,group = method)) +
  geom_point()+ geom_line() +  facet_wrap('~task') + theme_bw() +
  scale_x_discrete(guide = guide_axis(angle = 45)) + theme(legend.position = "bottom")
```

## CiLISI score

```{r}
data <- metricsR[grepl(metricsR$annotations,pattern = "broad|original")&metricsR$task %in% c('immune_cell_hum','tcells_atlas'),]

ggplot(data,aes(x = annotations,color=method,y=CiLISI,group = method)) +
  geom_point()+ geom_line() +  facet_wrap('~task') + theme_bw() +
  scale_x_discrete(guide = guide_axis(angle = 45)) + theme(legend.position = "bottom")
```


# CiLISI vs ASW label scatter plots

```{r}
rownames(metricsR) <- gsub(rownames(metricsR),pattern = "/R/",replacement = "/")
allMetrics <- cbind(metrics,metricsR[rownames(metrics),c("CiLISI","CiLISI_means")])
```


## Label shuflling and partial annotation

```{r fig.height=5,fig.width=10}

for (t in unique(allMetrics$task)) {
  metricsTask <- allMetrics[allMetrics$task == t,]
  # plot(ggplot(metricsTask[grepl(metricsTask$annotations,pattern = "broad|original"),],aes(x = CiLISI, y= ASW_label_raw, color = method, group = method)) +geom_point() + facet_wrap(c("task","annotations"), ncol = 5)+theme(legend.position = "bottom"))
  plot(ggplot(metricsTask[grepl(metricsTask$annotations,pattern = "unknown|original"),],
              aes(x = CiLISI, y= ASW_label_raw, color = method, group = method)) +
         geom_point() + facet_wrap(c("task","annotations"), ncol = 5)+theme(legend.position = "bottom"))
  plot(ggplot(metricsTask[grepl(metricsTask$annotations,pattern = "shuffled|original"),],
              aes(x = CiLISI, y= ASW_label_raw, color = method, group = method)) +
         geom_point() + facet_wrap(c("task","annotations"), ncol = 5)+theme(legend.position = "bottom"))
}
```
## Broad annotations

```{r}
for (t in c("immune_cell_hum","tcells_atlas")) {
  metricsTask <- allMetrics[allMetrics$task == t,]
  plot(ggplot(metricsTask[grepl(metricsTask$annotations,pattern = "broad|original"),],
              aes(x = CiLISI, y= ASW_label_raw, color = method, group = method)) +
         geom_point() + facet_wrap(c("task","annotations"), ncol = 5)+
         theme(legend.position = "bottom"))
  # plot(ggplot(metricsTask[grepl(metricsTask$annotations,pattern = "unknown|None"),],aes(x = CiLISI, y= ASW_label_raw, color = method, group = method)) +geom_point() + facet_wrap(c("task","annotations"), ncol = 5)+theme(legend.position = "bottom"))
  # plot(ggplot(metricsTask[grepl(metricsTask$annotations,pattern = "shuffled|None"),],aes(x = CiLISI, y= ASW_label_raw, color = method, group = method)) +geom_point() + facet_wrap(c("task","annotations"), ncol = 5)+theme(legend.position = "bottom"))
}
```

## Compute true mislabelling rates
This chunk requires the preprocessed anndata objects `"../../supervised_tool_analysis/*/prepare/unscaled/hvg/adata_pre.h5ad` for each integration task generated with the `BenchmarkingSupervisedTool` snakemake pipeline.
```{r eval=F}
misRateTable <- data.frame()
celltypeLabel = c("celltype","celltype","final_annotation","cell_type")
names(celltypeLabel) <- c('tcells_atlas','human_pancreas','immune_cell_hum',"lung_atlas")

atlases <- c('tcells_atlas','human_pancreas','immune_cell_hum',"lung_atlas")

for (task in atlases) {
  adata <- read_h5ad(paste0("../../supervised_tool_analysis/",task,"/prepare/unscaled/hvg/adata_pre.h5ad"))
  for (p in c(10,20,50,100)) {
    misRate <- table(adata$obs[,paste0('shuffled_',p,'_',celltypeLabel[task])] == adata$obs[,celltypeLabel[task]])[1]/length(adata$obs[,celltypeLabel[task]])
    misRateTable <- rbind(misRateTable,c(task, p, misRate))
  }
}
colnames(misRateTable) <- c("task","shuffled","mislabelled")
misRateTable$mislabelled <- as.numeric(misRateTable$mislabelled)

misRateTable$shuffled <- as.numeric(misRateTable$shuffled)
ggplot(misRateTable, aes(x = shuffled , y = mislabelled, fill = task)) + geom_bar(stat="identity", color="black", position=position_dodge())
```





